{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "# Accessing Text from the Web and from Disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Electronic Books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib import request\n",
    "\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "response = request.urlopen(url)\n",
    "raw = response.read().decode(\"utf8\")\n",
    "type(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1176967"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'\\ufeffThe Project Gutenberg EBook of Crime and Punishment, by Fyodor Dostoevsky\\r'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(raw)\n",
    "\n",
    "raw[:75]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "257085"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['\\ufeffThe',\n",
       " 'Project',\n",
       " 'Gutenberg',\n",
       " 'EBook',\n",
       " 'of',\n",
       " 'Crime',\n",
       " 'and',\n",
       " 'Punishment',\n",
       " ',',\n",
       " 'by']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tokenizing raw strings into words and punctuations\n",
    "tokens = word_tokenize(raw)\n",
    "type(tokens)\n",
    "len(tokens)\n",
    "tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nltk.text.Text"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['I',\n",
       " 'CHAPTER',\n",
       " 'I',\n",
       " 'On',\n",
       " 'an',\n",
       " 'exceptionally',\n",
       " 'hot',\n",
       " 'evening',\n",
       " 'early',\n",
       " 'in',\n",
       " 'July',\n",
       " 'a',\n",
       " 'young',\n",
       " 'man',\n",
       " 'came',\n",
       " 'out',\n",
       " 'of',\n",
       " 'the',\n",
       " 'garret',\n",
       " 'in',\n",
       " 'which',\n",
       " 'he',\n",
       " 'lodged',\n",
       " 'in',\n",
       " 'S.',\n",
       " 'Place',\n",
       " 'and',\n",
       " 'walked',\n",
       " 'slowly',\n",
       " ',',\n",
       " 'as',\n",
       " 'though',\n",
       " 'in',\n",
       " 'hesitation',\n",
       " ',',\n",
       " 'towards',\n",
       " 'K.',\n",
       " 'bridge']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Katerina Ivanovna; Pyotr Petrovitch; Pulcheria Alexandrovna; Avdotya\n",
      "Romanovna; Rodion Romanovitch; Marfa Petrovna; Sofya Semyonovna; old\n",
      "woman; Project Gutenberg-tm; Porfiry Petrovitch; Amalia Ivanovna;\n",
      "great deal; young man; Nikodim Fomitch; Ilya Petrovitch; Project\n",
      "Gutenberg; Andrey Semyonovitch; Hay Market; Dmitri Prokofitch; Good\n",
      "heavens\n"
     ]
    }
   ],
   "source": [
    "# creating a NLTK text\n",
    "text = nltk.Text(tokens)\n",
    "type(text)\n",
    "text[1024:1062]\n",
    "text.collocations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5336"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Need to remove footers in every Project Gutenberg files to only use original text\n",
    "# using visual inspection\n",
    "raw.find(\"PART I\")\n",
    "raw.rfind(\"End of Project Gutenberg's Crime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = raw[5336:1157743]\n",
    "raw.find(\"PART I\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dealing with HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!doctype html public \"-//W3C//DTD HTML 4.0 Transitional//EN'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"http://news.bbc.co.uk/2/hi/health/2284783.stm\"\n",
    "html = request.urlopen(url).read().decode(\"utf8\")\n",
    "html[:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BBC',\n",
       " 'NEWS',\n",
       " '|',\n",
       " 'Health',\n",
       " '|',\n",
       " 'Blondes',\n",
       " \"'to\",\n",
       " 'die',\n",
       " 'out',\n",
       " 'in',\n",
       " '200',\n",
       " \"years'\",\n",
       " 'NEWS',\n",
       " 'SPORT',\n",
       " 'WEATHER',\n",
       " 'WORLD',\n",
       " 'SERVICE',\n",
       " 'A-Z',\n",
       " 'INDEX',\n",
       " 'SEARCH']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "raw = BeautifulSoup(html, \"html.parser\").get_text()\n",
    "tokens = word_tokenize(raw)\n",
    "tokens[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 5 of 5 matches:\n",
      "hey say too few people now carry the gene for blondes to last beyond the next \n",
      "blonde hair is caused by a recessive gene . In order for a child to have blond\n",
      " have blonde hair , it must have the gene on both sides of the family in the g\n",
      "ere is a disadvantage of having that gene or by chance . They do n't disappear\n",
      "des would disappear is if having the gene was a disadvantage and I do not thin\n"
     ]
    }
   ],
   "source": [
    "# Remove unwanted content regarding site navigation\n",
    "tokens = tokens[110:390]\n",
    "text = nltk.Text(tokens)\n",
    "text.concordance(\"gene\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing RSS Feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Language Log'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import feedparser\n",
    "\n",
    "llog = feedparser.parse(\"http://languagelog.ldc.upenn.edu/nll/?feed=atom\")\n",
    "llog[\"feed\"][\"title\"]\n",
    "len(llog.entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The computational linguistics of COVID-19 vaccine design'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post = llog.entries[2]\n",
    "post.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p>He Zhang, Liang Zhang, Ziyu Li, Kaibo Liu, Boxiang Liu, David H. Ma'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = post.content[0].value\n",
    "content[:70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He', 'Zhang', ',', 'Liang', 'Zhang', ',', 'Ziyu', 'Li', ',', 'Kaibo']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw = BeautifulSoup(content, \"html.parser\").get_text()\n",
    "word_tokenize(raw)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading Local files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Capturing User input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-79c7dfc13144>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter some text: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You typed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"words.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nltk-book-FGAGMkET-py3.8/lib/python3.8/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    859\u001b[0m             )\n\u001b[0;32m--> 860\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    861\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Library/Caches/pypoetry/virtualenvs/nltk-book-FGAGMkET-py3.8/lib/python3.8/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    902\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 904\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "s = input(\"Enter some text: \")\n",
    "print(\"You typed\", len(word_tokenize(s)), \"words.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The NLP Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Strings: Text Processing at the Lowest Level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Operations with Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Monty Python'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "monty = \"Monty Python\"\n",
    "monty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Monty Python's Flying Circus\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "circus = \"Monty Python's Flying Circus\"\n",
    "circus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Monty Python's Flying Circus\""
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "circus = \"Monty Python's Flying Circus\"\n",
    "circus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-20-b5a5bc20944b>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-20-b5a5bc20944b>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    circus = 'Monty Python's Flying Circus'\u001b[0m\n\u001b[0m                           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "circus = 'Monty Python's Flying Circus'\n",
    "circus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Shall I compare thee to a Summer's day?Thou are more lovely and more temperate:\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "couplet = (\n",
    "    \"Shall I compare thee to a Summer's day?\" \"Thou are more lovely and more temperate:\"\n",
    ")\n",
    "couplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Shall I compare thee to a Summer's day? Thou are more lovely and more temperate:\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "couplet = (\n",
    "    \"Shall I compare thee to a Summer's day? Thou are more lovely and more temperate:\"\n",
    ")\n",
    "couplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Shall I compare thee to a Summer's day?\\nThou are more lovely and more temperate:\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "couplet = \"\"\"Shall I compare thee to a Summer's day?\n",
    "Thou are more lovely and more temperate:\"\"\"\n",
    "couplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Shall I compare thee to a Summer's day?\\nThou are more lovely and more temperate:\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "couplet = \"\"\"Shall I compare thee to a Summer's day?\n",
    "Thou are more lovely and more temperate:\"\"\"\n",
    "couplet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"very\" + \"very\" + \"very\"\n",
    "\n",
    "\"very\" * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'str' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-3d3a5347f2dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m\"very\"\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m\"y\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'str' and 'str'"
     ]
    }
   ],
   "source": [
    "\"very\" - \"y\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-ce10ba541511>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m\"very\"\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "\"very\" / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Printing Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(monty)\n",
    "\n",
    "grail = \"Holy Grail\"\n",
    "print(monty + grail)\n",
    "print(monty, grail)\n",
    "print(monty, \"and the\", grail)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing Individual Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "monty[0]\n",
    "monty[3]\n",
    "monty[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-573d88a40a56>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmonty\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "monty[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "monty[-1]\n",
    "monty[5]\n",
    "monty[-7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c o l o r l e s s   g r e e n   i d e a s   s l e e p   f u r i o u s l y "
     ]
    }
   ],
   "source": [
    "sent = \"colorless green ideas sleep furiously\"\n",
    "for char in sent:\n",
    "    print(char, end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "\n",
    "raw = gutenberg.raw(\"melville-moby_dick.txt\")\n",
    "fdist = nltk.FreqDist(ch.lower() for ch in raw if ch.isalpha())\n",
    "fdist.most_common(5)\n",
    "\n",
    "[char for (char, count) in fdist.most_common()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing Substrings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "monty[6:10]\n",
    "monty[-12:-7]\n",
    "monty[:5]\n",
    "monty[6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "phrase = \"And now for something completely different\"\n",
    "if \"thing\" in phrase:\n",
    "    print(\"found thing\")\n",
    "\n",
    "monty.find(\"Python\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More operations on strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Difference betweek lists and strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Who knows?\"\n",
    "beatles = [\"John\", \"Paul\", \"George\", \"Ringo\"]\n",
    "query[2]\n",
    "\n",
    "beatles[2]\n",
    "\n",
    "query[:2]\n",
    "\n",
    "beatles[:2]\n",
    "\n",
    "query + \"I don't\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate list (not \"str\") to list",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-c5f5c772228b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbeatles\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"Brian\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate list (not \"str\") to list"
     ]
    }
   ],
   "source": [
    "beatles + \"Brian\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "beatles + [\"Brian\"]\n",
    "\n",
    "beatles[0] = \"John Lennon\"\n",
    "\n",
    "del beatles[-1]\n",
    "\n",
    "beatles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-59fa3b41243b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mquery\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"F\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'str' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "query[0] = \"F\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Expressions for Detecting Word Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "wordlist = [w for w in nltk.corpus.words.words(\"en\") if w.islower()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using Basic Meta-Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "[w for w in wordlist if re.search(\"ed$\", w)]\n",
    "\n",
    "[w for w in wordlist if re.search(\"^..j..t..$\", w)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ranges and Closures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gold', 'golf', 'hold', 'hole']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w for w in wordlist if re.search(\"^[ghi][mno][jlk][def]$\", w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_words = sorted(set(w for w in nltk.corpus.nps_chat.words()))\n",
    "[w for w in chat_words if re.search(\"^m+i+n+e+$\", w)]\n",
    "\n",
    "[w for w in chat_words if re.search(\"^[ha]+$\", w)]\n",
    "\n",
    "[w for w in chat_words if re.search(\"^m*i*n*e*$\", w)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "wsj = sorted(set(nltk.corpus.treebank.words()))\n",
    "[w for w in wsj if re.search(\"^[0-9]+\\.[0-9]+$\", w)]\n",
    "\n",
    "[w for w in wsj if re.search(\"^[A-Z]+\\$$\", w)]\n",
    "\n",
    "[w for w in wsj if re.search(\"^[0-9]{4}$\", w)]\n",
    "\n",
    "[w for w in wsj if re.search(\"^[0-9]+-[a-z]{3,5}$\", w)]\n",
    "\n",
    "[w for w in wsj if re.search(\"^[[a-z]{5,}-[a-z]{2,3}-[a-z]{,6}$\", w)]\n",
    "\n",
    "[w for w in wsj if re.search(\"(ed|ing)$\", w)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Useful Application of Regular Expression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting Word Pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['u',\n",
       " 'e',\n",
       " 'a',\n",
       " 'i',\n",
       " 'a',\n",
       " 'i',\n",
       " 'i',\n",
       " 'i',\n",
       " 'e',\n",
       " 'i',\n",
       " 'a',\n",
       " 'i',\n",
       " 'o',\n",
       " 'i',\n",
       " 'o',\n",
       " 'u']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"supercalifragilisticexpialidocious\"\n",
    "re.findall(r\"[aeiou]\", word)\n",
    "\n",
    "len(re.findall(r\"[aeiou]\", word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('io', 549),\n",
       " ('ea', 476),\n",
       " ('ie', 331),\n",
       " ('ou', 329),\n",
       " ('ai', 261),\n",
       " ('ia', 253),\n",
       " ('ee', 217),\n",
       " ('oo', 174),\n",
       " ('ua', 109),\n",
       " ('au', 106),\n",
       " ('ue', 105),\n",
       " ('ui', 95)]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsj = sorted(set(nltk.corpus.treebank.words()))\n",
    "fd = nltk.FreqDist(vs for word in wsj for vs in re.findall(r\"[aeiou]{2,}\", word))\n",
    "fd.most_common(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Doing more with Word Pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unvrsl Dclrtn of Hmn Rghts Prmble Whrs rcgntn of the inhrnt dgnty and\n",
      "of the eql and inlnble rghts of all mmbrs of the hmn fmly is the fndtn\n",
      "of frdm , jstce and pce in the wrld , Whrs dsrgrd and cntmpt fr hmn\n",
      "rghts hve rsltd in brbrs acts whch hve outrgd the cnscnce of mnknd ,\n",
      "and the advnt of a wrld in whch hmn bngs shll enjy frdm of spch and\n"
     ]
    }
   ],
   "source": [
    "regexp = r\"^[AEIOUaeiou]+|[AEIOUaeiou]+$|[^AEIOUaeiou]+\"\n",
    "\n",
    "\n",
    "def compress(word):\n",
    "    pieces = re.findall(regexp, word)\n",
    "    return \"\".join(pieces)\n",
    "\n",
    "\n",
    "english_udhr = nltk.corpus.udhr.words(\"English-Latin1\")\n",
    "print(nltk.tokenwrap(compress(w) for w in english_udhr[:75]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    a   e   i   o   u \n",
      "k 418 148  94 420 173 \n",
      "p  83  31 105  34  51 \n",
      "r 187  63  84  89  79 \n",
      "s   0   0 100   2   1 \n",
      "t  47   8   0 148  37 \n",
      "v  93  27 105  48  49 \n"
     ]
    }
   ],
   "source": [
    "rotokas_words = nltk.corpus.toolbox.words(\"rotokas.dic\")\n",
    "cvs = [cv for w in rotokas_words for cv in re.findall(r\"[ptksvr][aeiou]\", w)]\n",
    "cfd = nltk.ConditionalFreqDist(cvs)\n",
    "cfd.tabulate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kasuari']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_word_pairs = [\n",
    "    (cv, w) for w in rotokas_words for cv in re.findall(r\"[ptksvr][aeiou]\", w)\n",
    "]\n",
    "cv_index = nltk.Index(cv_word_pairs)\n",
    "cv_index[\"su\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['kaapo',\n",
       " 'kaapopato',\n",
       " 'kaipori',\n",
       " 'kaiporipie',\n",
       " 'kaiporivira',\n",
       " 'kapo',\n",
       " 'kapoa',\n",
       " 'kapokao',\n",
       " 'kapokapo',\n",
       " 'kapokapo',\n",
       " 'kapokapoa',\n",
       " 'kapokapoa',\n",
       " 'kapokapora',\n",
       " 'kapokapora',\n",
       " 'kapokaporo',\n",
       " 'kapokaporo',\n",
       " 'kapokari',\n",
       " 'kapokarito',\n",
       " 'kapokoa',\n",
       " 'kapoo',\n",
       " 'kapooto',\n",
       " 'kapoovira',\n",
       " 'kapopaa',\n",
       " 'kaporo',\n",
       " 'kaporo',\n",
       " 'kaporopa',\n",
       " 'kaporoto',\n",
       " 'kapoto',\n",
       " 'karokaropo',\n",
       " 'karopo',\n",
       " 'kepo',\n",
       " 'kepoi',\n",
       " 'keposi',\n",
       " 'kepoto']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_index[\"po\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding word stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stem(word):\n",
    "    for suffix in [\"ing\", \"ly\", \"ed\", \"ious\", \"ies\", \"ive\", \"es\", \"s\", \"ment\"]:\n",
    "        if word.endswith(suffix):\n",
    "            return word[: -len(suffix)]\n",
    "    return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ing']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['processing']"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('processe', 's')]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('process', 'es')]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "[('language', '')]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"^.*(ing|ly|ed|ious|ies|ive|es|s|ment)$\", \"processing\")\n",
    "re.findall(r\"^.*(?:ing|ly|ed|ious|ies|ive|es|s|ment)$\", \"processing\")\n",
    "re.findall(r\"^(.*)(ing|ly|ed|ious|ies|ive|es|s|ment)$\", \"processes\")\n",
    "re.findall(r\"^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)$\", \"processes\")\n",
    "re.findall(r\"^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$\", \"language\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DENNIS',\n",
       " ':',\n",
       " 'Listen',\n",
       " ',',\n",
       " 'strange',\n",
       " 'women',\n",
       " 'ly',\n",
       " 'in',\n",
       " 'pond',\n",
       " 'distribut',\n",
       " 'sword',\n",
       " 'i',\n",
       " 'no',\n",
       " 'basi',\n",
       " 'for',\n",
       " 'a',\n",
       " 'system',\n",
       " 'of',\n",
       " 'govern',\n",
       " '.',\n",
       " 'Supreme',\n",
       " 'execut',\n",
       " 'power',\n",
       " 'deriv',\n",
       " 'from',\n",
       " 'a',\n",
       " 'mandate',\n",
       " 'from',\n",
       " 'the',\n",
       " 'mass',\n",
       " ',',\n",
       " 'not',\n",
       " 'from',\n",
       " 'some',\n",
       " 'farcical',\n",
       " 'aquatic',\n",
       " 'ceremony',\n",
       " '.']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def stem(word):\n",
    "    regexp = r\"^(.*?)(ing|ly|ed|ious|ies|ive|es|s|ment)?$\"\n",
    "    stem, suffix = re.findall(regexp, word)[0]\n",
    "    return stem\n",
    "\n",
    "\n",
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords\n",
    "is no basis for a system of government.  Supreme executive power derives from\n",
    "a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "tokens = word_tokenize(raw)\n",
    "[stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Searching Tokenised Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monied; nervous; dangerous; white; white; white; pious; queer; good;\n",
      "mature; white; Cape; great; wise; wise; butterless; white; fiendish;\n",
      "pale; furious; better; certain; complete; dismasted; younger; brave;\n",
      "brave; brave; brave\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg, nps_chat\n",
    "\n",
    "moby = nltk.Text(gutenberg.words(\"melville-moby_dick.txt\"))\n",
    "moby.findall(r\"<a> (<.*>) <man>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "you rule bro; telling you bro; u twizted bro\n",
      "lol lol lol; lmao lol lol; lol lol lol; la la la la la; la la la; la\n",
      "la la; lovely lol lol love; lol lol lol.; la la la; la la la\n"
     ]
    }
   ],
   "source": [
    "chat = nltk.Text(nps_chat.words())\n",
    "chat.findall(r\"<.*> <.*> <bro>\")\n",
    "\n",
    "chat.findall(r\"<l.*>{3,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "speed and other activities; water and other liquids; tomb and other\n",
      "landmarks; Statues and other monuments; pearls and other jewels;\n",
      "charts and other items; roads and other features; figures and other\n",
      "objects; military and other areas; demands and other factors;\n",
      "abstracts and other compilations; iron and other metals\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "hobbies_learned = nltk.Text(brown.words(categories=[\"hobbies\", \"learned\"]))\n",
    "hobbies_learned.findall(r\"<\\w*> <and> <other> <\\w*s>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = \"\"\"DENNIS: Listen, strange women lying in ponds distributing swords \n",
    "is no basis for a system of government.  Supreme executive power derives from \n",
    "a mandate from the masses, not from some farcical aquatic ceremony.\"\"\"\n",
    "\n",
    "tokens = word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemmers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['denni',\n",
       " ':',\n",
       " 'listen',\n",
       " ',',\n",
       " 'strang',\n",
       " 'women',\n",
       " 'lie',\n",
       " 'in',\n",
       " 'pond',\n",
       " 'distribut',\n",
       " 'sword',\n",
       " 'is',\n",
       " 'no',\n",
       " 'basi',\n",
       " 'for',\n",
       " 'a',\n",
       " 'system',\n",
       " 'of',\n",
       " 'govern',\n",
       " '.',\n",
       " 'suprem',\n",
       " 'execut',\n",
       " 'power',\n",
       " 'deriv',\n",
       " 'from',\n",
       " 'a',\n",
       " 'mandat',\n",
       " 'from',\n",
       " 'the',\n",
       " 'mass',\n",
       " ',',\n",
       " 'not',\n",
       " 'from',\n",
       " 'some',\n",
       " 'farcic',\n",
       " 'aquat',\n",
       " 'ceremoni',\n",
       " '.']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['den',\n",
       " ':',\n",
       " 'list',\n",
       " ',',\n",
       " 'strange',\n",
       " 'wom',\n",
       " 'lying',\n",
       " 'in',\n",
       " 'pond',\n",
       " 'distribut',\n",
       " 'sword',\n",
       " 'is',\n",
       " 'no',\n",
       " 'bas',\n",
       " 'for',\n",
       " 'a',\n",
       " 'system',\n",
       " 'of',\n",
       " 'govern',\n",
       " '.',\n",
       " 'suprem',\n",
       " 'execut',\n",
       " 'pow',\n",
       " 'der',\n",
       " 'from',\n",
       " 'a',\n",
       " 'mand',\n",
       " 'from',\n",
       " 'the',\n",
       " 'mass',\n",
       " ',',\n",
       " 'not',\n",
       " 'from',\n",
       " 'som',\n",
       " 'farc',\n",
       " 'aqu',\n",
       " 'ceremony',\n",
       " '.']"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "[porter.stem(t) for t in tokens]\n",
    "\n",
    "[lancaster.stem(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r king ! DENNIS : Listen , strange women lying in ponds distributing swords is no\n",
      " beat a very brave retreat . ROBIN : All lies ! MINSTREL : [ singing ] Bravest of\n",
      "       Nay . Nay . Come . Come . You may lie here . Oh , but you are wounded !   \n",
      "doctors immediately ! No , no , please ! Lie down . [ clap clap ] PIGLET : Well  \n",
      "ere is much danger , for beyond the cave lies the Gorge of Eternal Peril , which \n",
      "   you . Oh ... TIM : To the north there lies a cave -- the cave of Caerbannog --\n",
      "h it and lived ! Bones of full fifty men lie strewn about its lair . So , brave k\n",
      "not stop our fight ' til each one of you lies dead , and the Holy Grail returns t\n"
     ]
    }
   ],
   "source": [
    "class IndexedText(object):\n",
    "    def __init__(self, stemmer, text):\n",
    "        self._text = text\n",
    "        self._stemmer = stemmer\n",
    "        self._index = nltk.Index((self._stem(word), i) for (i, word) in enumerate(text))\n",
    "\n",
    "    def concordance(self, word, width=40):\n",
    "        key = self._stem(word)\n",
    "        wc = int(width / 4)  # words of context\n",
    "        for i in self._index[key]:\n",
    "            lcontext = \" \".join(self._text[i - wc : i])\n",
    "            rcontext = \" \".join(self._text[i : i + wc])\n",
    "            ldisplay = \"{:>{width}}\".format(lcontext[-width:], width=width)\n",
    "            rdisplay = \"{:{width}}\".format(rcontext[:width], width=width)\n",
    "            print(ldisplay, rdisplay)\n",
    "\n",
    "    def _stem(self, word):\n",
    "        return self._stemmer.stem(word).lower()\n",
    "\n",
    "\n",
    "porter = nltk.PorterStemmer()\n",
    "grail = nltk.corpus.webtext.words(\"grail.txt\")\n",
    "text = IndexedText(porter, grail)\n",
    "text.concordance(\"lie\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DENNIS',\n",
       " ':',\n",
       " 'Listen',\n",
       " ',',\n",
       " 'strange',\n",
       " 'woman',\n",
       " 'lying',\n",
       " 'in',\n",
       " 'pond',\n",
       " 'distributing',\n",
       " 'sword',\n",
       " 'is',\n",
       " 'no',\n",
       " 'basis',\n",
       " 'for',\n",
       " 'a',\n",
       " 'system',\n",
       " 'of',\n",
       " 'government',\n",
       " '.',\n",
       " 'Supreme',\n",
       " 'executive',\n",
       " 'power',\n",
       " 'derives',\n",
       " 'from',\n",
       " 'a',\n",
       " 'mandate',\n",
       " 'from',\n",
       " 'the',\n",
       " 'mass',\n",
       " ',',\n",
       " 'not',\n",
       " 'from',\n",
       " 'some',\n",
       " 'farcical',\n",
       " 'aquatic',\n",
       " 'ceremony',\n",
       " '.']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "[wnl.lemmatize(t) for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regular Expression for Tokenizing Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = \"\"\"'When I'M a Duchess,' she said to herself, (not in a very hopeful tone though), \n",
    "'I won't have any pepper in my kitchen AT ALL. Soup does very \n",
    "well without--Maybe it's always pepper that makes people hot-tempered,'...\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Approaches to Tokenisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'When\",\n",
       " \"I'M\",\n",
       " 'a',\n",
       " \"Duchess,'\",\n",
       " 'she',\n",
       " 'said',\n",
       " 'to',\n",
       " 'herself,',\n",
       " '(not',\n",
       " 'in',\n",
       " 'a',\n",
       " 'very',\n",
       " 'hopeful',\n",
       " 'tone',\n",
       " 'though),',\n",
       " \"\\n'I\",\n",
       " \"won't\",\n",
       " 'have',\n",
       " 'any',\n",
       " 'pepper',\n",
       " 'in',\n",
       " 'my',\n",
       " 'kitchen',\n",
       " 'AT',\n",
       " 'ALL.',\n",
       " 'Soup',\n",
       " 'does',\n",
       " 'very',\n",
       " '\\nwell',\n",
       " 'without--Maybe',\n",
       " \"it's\",\n",
       " 'always',\n",
       " 'pepper',\n",
       " 'that',\n",
       " 'makes',\n",
       " 'people',\n",
       " \"hot-tempered,'...\"]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(r\" \", raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'When\",\n",
       " \"I'M\",\n",
       " 'a',\n",
       " \"Duchess,'\",\n",
       " 'she',\n",
       " 'said',\n",
       " 'to',\n",
       " 'herself,',\n",
       " '(not',\n",
       " 'in',\n",
       " 'a',\n",
       " 'very',\n",
       " 'hopeful',\n",
       " 'tone',\n",
       " 'though),',\n",
       " \"'I\",\n",
       " \"won't\",\n",
       " 'have',\n",
       " 'any',\n",
       " 'pepper',\n",
       " 'in',\n",
       " 'my',\n",
       " 'kitchen',\n",
       " 'AT',\n",
       " 'ALL.',\n",
       " 'Soup',\n",
       " 'does',\n",
       " 'very',\n",
       " 'well',\n",
       " 'without--Maybe',\n",
       " \"it's\",\n",
       " 'always',\n",
       " 'pepper',\n",
       " 'that',\n",
       " 'makes',\n",
       " 'people',\n",
       " \"hot-tempered,'...\"]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(r\"[ \\t\\n]+\", raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['',\n",
       " 'When',\n",
       " 'I',\n",
       " 'M',\n",
       " 'a',\n",
       " 'Duchess',\n",
       " 'she',\n",
       " 'said',\n",
       " 'to',\n",
       " 'herself',\n",
       " 'not',\n",
       " 'in',\n",
       " 'a',\n",
       " 'very',\n",
       " 'hopeful',\n",
       " 'tone',\n",
       " 'though',\n",
       " 'I',\n",
       " 'won',\n",
       " 't',\n",
       " 'have',\n",
       " 'any',\n",
       " 'pepper',\n",
       " 'in',\n",
       " 'my',\n",
       " 'kitchen',\n",
       " 'AT',\n",
       " 'ALL',\n",
       " 'Soup',\n",
       " 'does',\n",
       " 'very',\n",
       " 'well',\n",
       " 'without',\n",
       " 'Maybe',\n",
       " 'it',\n",
       " 's',\n",
       " 'always',\n",
       " 'pepper',\n",
       " 'that',\n",
       " 'makes',\n",
       " 'people',\n",
       " 'hot',\n",
       " 'tempered',\n",
       " '']"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.split(r\"\\W+\", raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"'When\",\n",
       " 'I',\n",
       " \"'M\",\n",
       " 'a',\n",
       " 'Duchess',\n",
       " ',',\n",
       " \"'\",\n",
       " 'she',\n",
       " 'said',\n",
       " 'to',\n",
       " 'herself',\n",
       " ',',\n",
       " '(not',\n",
       " 'in',\n",
       " 'a',\n",
       " 'very',\n",
       " 'hopeful',\n",
       " 'tone',\n",
       " 'though',\n",
       " ')',\n",
       " ',',\n",
       " \"'I\",\n",
       " 'won',\n",
       " \"'t\",\n",
       " 'have',\n",
       " 'any',\n",
       " 'pepper',\n",
       " 'in',\n",
       " 'my',\n",
       " 'kitchen',\n",
       " 'AT',\n",
       " 'ALL',\n",
       " '.',\n",
       " 'Soup',\n",
       " 'does',\n",
       " 'very',\n",
       " 'well',\n",
       " 'without',\n",
       " '-',\n",
       " '-Maybe',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'always',\n",
       " 'pepper',\n",
       " 'that',\n",
       " 'makes',\n",
       " 'people',\n",
       " 'hot',\n",
       " '-tempered',\n",
       " ',',\n",
       " \"'\",\n",
       " '.',\n",
       " '.',\n",
       " '.']"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r\"\\w+|\\S\\w*\", raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'\", 'When', \"I'M\", 'a', 'Duchess', ',', \"'\", 'she', 'said', 'to', 'herself', ',', '(', 'not', 'in', 'a', 'very', 'hopeful', 'tone', 'though', ')', ',', \"'\", 'I', \"won't\", 'have', 'any', 'pepper', 'in', 'my', 'kitchen', 'AT', 'ALL', '.', 'Soup', 'does', 'very', 'well', 'without', '--', 'Maybe', \"it's\", 'always', 'pepper', 'that', 'makes', 'people', 'hot-tempered', ',', \"'\", '...']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(r\"\\w+(?:[-']\\w+)*|'|[-.(]+|\\S\\w*\", raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK's Regular Expression Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['That', 'U.S.A.', 'poster-print', 'costs', '$12.40', '...']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"That U.S.A. poster-print costs $12.40...\"\n",
    "pattern = r\"\"\"(?x)     # set flag to allow verbose regexps\n",
    "     (?:[A-Z]\\.)+       # abbreviations, e.g. U.S.A.\n",
    "   | \\w+(?:-\\w+)*       # words with optional internal hyphens\n",
    "   | \\$?\\d+(?:\\.\\d+)?%? # currency and percentages, e.g. $12.40, 82%\n",
    "   | \\.\\.\\.             # ellipsis\n",
    "   | [][.,;\"'?():-_`]   # these are separate tokens; includes ], [\n",
    "\"\"\"\n",
    "nltk.regexp_tokenize(text, pattern)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentence Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.250994070456922"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nltk.corpus.brown.words()) / len(nltk.corpus.brown.sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"Nonsense!\"',\n",
      " 'said Gregory, who was very rational when anyone else\\nattempted paradox.',\n",
      " '\"Why do all the clerks and navvies in the\\n'\n",
      " 'railway trains look so sad and tired, so very sad and tired?',\n",
      " 'I will\\ntell you.',\n",
      " 'It is because they know that the train is going right.',\n",
      " 'It\\n'\n",
      " 'is because they know that whatever place they have taken a ticket\\n'\n",
      " 'for that place they will reach.',\n",
      " 'It is because after they have\\n'\n",
      " 'passed Sloane Square they know that the next station must be\\n'\n",
      " 'Victoria, and nothing but Victoria.',\n",
      " 'Oh, their wild rapture!',\n",
      " 'oh,\\n'\n",
      " 'their eyes like stars and their souls again in Eden, if the next\\n'\n",
      " 'station were unaccountably Baker Street!\"',\n",
      " '\"It is you who are unpoetical,\" replied the poet Syme.']\n"
     ]
    }
   ],
   "source": [
    "text = nltk.corpus.gutenberg.raw(\"chesterton-thursday.txt\")\n",
    "sents = nltk.sent_tokenize(text)\n",
    "pprint.pprint(sents[79:89])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
    "seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n",
    "seg2 = \"0100100100100001001001000010100100010010000100010010000\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment(text, segs):\n",
    "    words = []\n",
    "    last = 0\n",
    "    for i in range(len(segs)):\n",
    "        if segs[i] == \"1\":\n",
    "            words.append(text[last : i + 1])\n",
    "            last = i + 1\n",
    "    words.append(text[last:])\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['do',\n",
       " 'you',\n",
       " 'see',\n",
       " 'the',\n",
       " 'kitty',\n",
       " 'see',\n",
       " 'the',\n",
       " 'doggy',\n",
       " 'do',\n",
       " 'you',\n",
       " 'like',\n",
       " 'the',\n",
       " 'kitty',\n",
       " 'like',\n",
       " 'the',\n",
       " 'doggy']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment(text, seg1)\n",
    "segment(text, seg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(text, segs):\n",
    "    words = segment(text, segs)\n",
    "    text_size = len(words)\n",
    "    lexicon_size = sum(len(word) + 1 for word in set(words))\n",
    "    return text_size + lexicon_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['doyou',\n",
       " 'see',\n",
       " 'thekitt',\n",
       " 'y',\n",
       " 'see',\n",
       " 'thedogg',\n",
       " 'y',\n",
       " 'doyou',\n",
       " 'like',\n",
       " 'thekitt',\n",
       " 'y',\n",
       " 'like',\n",
       " 'thedogg',\n",
       " 'y']"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "47"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "48"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
    "seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n",
    "seg2 = \"0100100100100001001001000010100100010010000100010010000\"\n",
    "seg3 = \"0000100100000011001000000110000100010000001100010000001\"\n",
    "segment(text, seg3)\n",
    "evaluate(text, seg3)\n",
    "evaluate(text, seg2)\n",
    "evaluate(text, seg1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "\n",
    "def flip(segs, pos):\n",
    "    return segs[:pos] + str(1 - int(segs[pos])) + segs[pos + 1 :]\n",
    "\n",
    "\n",
    "def flip_n(segs, n):\n",
    "    for i in range(n):\n",
    "        segs = flip(segs, randint(0, len(segs) - 1))\n",
    "    return segs\n",
    "\n",
    "\n",
    "def anneal(text, segs, iterations, cooling_rate):\n",
    "    temperature = float(len(segs))\n",
    "    while temperature > 0.5:\n",
    "        best_segs, best = segs, evaluate(text, segs)\n",
    "        for i in range(iterations):\n",
    "            guess = flip_n(segs, round((temperature)))\n",
    "            score = evaluate(text, guess)\n",
    "            if score < best:\n",
    "                best, best_segs = score, guess\n",
    "        score, segs = best, best_segs\n",
    "        temperature = temperature / cooling_rate\n",
    "        print(evaluate(text, segs), segment(text, segs))\n",
    "    print()\n",
    "    return segs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "64 ['doyouseethekitty', 'seethedoggy', 'doyoulikethekitty', 'likethedoggy']\n",
      "63 ['doyo', 'u', 'seeth', 'ek', 'itty', 'seeth', 'edoggy', 'doyo', 'uliketh', 'ek', 'itty', 'l', 'ikethedoggy']\n",
      "63 ['doyo', 'u', 'seeth', 'ek', 'itty', 'seeth', 'edoggy', 'doyo', 'uliketh', 'ek', 'itty', 'l', 'ikethedoggy']\n",
      "60 ['doyo', 'u', 'seeth', 'ekit', 'ty', 'seeth', 'edoggy', 'doyo', 'ul', 'iketh', 'ek', 'ittyl', 'iketh', 'edoggy']\n",
      "60 ['doyo', 'u', 'seeth', 'ekit', 'ty', 'seeth', 'edoggy', 'doyo', 'ul', 'iketh', 'ek', 'ittyl', 'iketh', 'edoggy']\n",
      "58 ['do', 'yo', 'u', 'seeth', 'ekitty', 'seeth', 'edoggy', 'doyo', 'ul', 'iketh', 'ekitty', 'l', 'iketh', 'edoggy']\n",
      "54 ['do', 'yo', 'u', 'seeth', 'ekitty', 'seeth', 'edoggy', 'do', 'yo', 'ul', 'iketh', 'ekitty', 'l', 'iketh', 'edoggy']\n",
      "54 ['do', 'yo', 'u', 'seeth', 'ekitty', 'seeth', 'edoggy', 'do', 'yo', 'ul', 'iketh', 'ekitty', 'l', 'iketh', 'edoggy']\n",
      "54 ['do', 'yo', 'u', 'seeth', 'ekitty', 'seeth', 'edoggy', 'do', 'yo', 'ul', 'iketh', 'ekitty', 'l', 'iketh', 'edoggy']\n",
      "52 ['do', 'yo', 'u', 'seeth', 'ekitty', 'seeth', 'edoggy', 'do', 'yo', 'u', 'l', 'iketh', 'ekitty', 'l', 'iketh', 'edoggy']\n",
      "49 ['do', 'yo', 'u', 'seeth', 'ekitty', 'seeth', 'edoggy', 'do', 'yo', 'u', 'liketh', 'ekitty', 'liketh', 'edoggy']\n",
      "46 ['do', 'you', 'seeth', 'ekitty', 'seeth', 'edoggy', 'do', 'you', 'liketh', 'ekitty', 'liketh', 'edoggy']\n",
      "43 ['doyou', 'seeth', 'ekitty', 'seeth', 'edoggy', 'doyou', 'liketh', 'ekitty', 'liketh', 'edoggy']\n",
      "43 ['doyou', 'seeth', 'ekitty', 'seeth', 'edoggy', 'doyou', 'liketh', 'ekitty', 'liketh', 'edoggy']\n",
      "43 ['doyou', 'seeth', 'ekitty', 'seeth', 'edoggy', 'doyou', 'liketh', 'ekitty', 'liketh', 'edoggy']\n",
      "43 ['doyou', 'seeth', 'ekitty', 'seeth', 'edoggy', 'doyou', 'liketh', 'ekitty', 'liketh', 'edoggy']\n",
      "43 ['doyou', 'seeth', 'ekitty', 'seeth', 'edoggy', 'doyou', 'liketh', 'ekitty', 'liketh', 'edoggy']\n",
      "43 ['doyou', 'seeth', 'ekitty', 'seeth', 'edoggy', 'doyou', 'liketh', 'ekitty', 'liketh', 'edoggy']\n",
      "43 ['doyou', 'seeth', 'ekitty', 'seeth', 'edoggy', 'doyou', 'liketh', 'ekitty', 'liketh', 'edoggy']\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0000100001000001000010000010000100000100000100000100000'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"doyouseethekittyseethedoggydoyoulikethekittylikethedoggy\"\n",
    "seg1 = \"0000000000000001000000000010000000000000000100000000000\"\n",
    "anneal(text, seg1, 5000, 1.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatting: From List to Strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Lists to Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'We called him Tortoise because he taught us .'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'We;called;him;Tortoise;because;he;taught;us;.'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'WecalledhimTortoisebecausehetaughtus.'"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "silly = [\"We\", \"called\", \"him\", \"Tortoise\", \"because\", \"he\", \"taught\", \"us\", \".\"]\n",
    "\" \".join(silly)\n",
    "\";\".join(silly)\n",
    "\"\".join(silly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strings and Formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat\n",
      "hello\n",
      "world\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'cat'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'hello\\nworld'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word = \"cat\"\n",
    "sentence = \"\"\"hello\n",
    "world\"\"\"\n",
    "print(word)\n",
    "print(sentence)\n",
    "word\n",
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat -> 3; dog -> 4; snake -> 1; "
     ]
    }
   ],
   "source": [
    "fdist = nltk.FreqDist([\"dog\", \"cat\", \"dog\", \"cat\", \"dog\", \"snake\", \"dog\", \"cat\"])\n",
    "for word in sorted(fdist):\n",
    "    print(word, \"->\", fdist[word], end=\"; \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cat -> 3; dog -> 4; snake -> 1; "
     ]
    }
   ],
   "source": [
    "for word in sorted(fdist):\n",
    "    print(f\"{word} -> {fdist[word]}\", end=\"; \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cat->3;'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'cat->'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'3'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'I want a coffee right now'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"{}->{};\".format(\"cat\", 3)\n",
    "\"{}->\".format(\"cat\")\n",
    "\"{}\".format(3)\n",
    "\"I want a {} right now\".format(\"coffee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lee wants a sandwich for lunch'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "IndexError",
     "evalue": "Replacement index 2 out of range for positional args tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-145-002a0cb8591a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m'{} wants a {} {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Lee'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sandwich'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'for lunch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;34m'{} wants a {} {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'sandwich'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'for lunch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: Replacement index 2 out of range for positional args tuple"
     ]
    }
   ],
   "source": [
    "\"{} wants a {} {}\".format(\"Lee\", \"sandwich\", \"for lunch\")\n",
    "\"{} wants a {} {}\".format(\"sandwich\", \"for lunch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Lee wants a sandwich'"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"{} wants a {}\".format(\"Lee\", \"sandwich\", \"for lunch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from B to A'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"from {1} to {0}\".format(\"A\", \"B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lee wants a sandwich right now\n",
      "Lee wants a spam fritter right now\n",
      "Lee wants a pancake right now\n"
     ]
    }
   ],
   "source": [
    "template = \"Lee wants a {} right now\"\n",
    "menu = [\"sandwich\", \"spam fritter\", \"pancake\"]\n",
    "for snack in menu:\n",
    "    print(template.format(snack))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lining Things up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'    41'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'41    '"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"{:6}\".format(41)\n",
    "\"{:<6}\".format(41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog   '"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'   dog'"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"{:6}\".format(\"dog\")\n",
    "\"{:>6}\".format(\"dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.1416'"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "\"{:.4f}\".format(math.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'accuracy for 9375 words: 34.1867%'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count, total = 3205, 9375\n",
    "f\"accuracy for {total} words: {count / total:.4%}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tabulate(cfdist, words, categories):\n",
    "    print(\"{:16}\".format(\"Category\"), end=\" \")  # column headings\n",
    "    for word in words:\n",
    "        print(\"{:>6}\".format(word), end=\" \")\n",
    "    print()\n",
    "    for category in categories:\n",
    "        print(\"{:16}\".format(category), end=\" \")  # row heading\n",
    "        for word in words:  # for each word\n",
    "            print(\"{:6}\".format(cfdist[category][word]), end=\" \")  # print table cell\n",
    "        print()  # end the row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category            can  could    may  might   must   will \n",
      "news                 93     86     66     38     50    389 \n",
      "religion             82     59     78     12     54     71 \n",
      "hobbies             268     58    131     22     83    264 \n",
      "science_fiction      16     49      4     12      8     16 \n",
      "romance              74    193     11     51     45     43 \n",
      "humor                16     30      8      8      9     13 \n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "\n",
    "cfd = nltk.ConditionalFreqDist(\n",
    "    (genre, word)\n",
    "    for genre in brown.categories()\n",
    "    for word in brown.words(categories=genre)\n",
    ")\n",
    "genres = [\"news\", \"religion\", \"hobbies\", \"science_fiction\", \"romance\", \"humor\"]\n",
    "modals = [\"can\", \"could\", \"may\", \"might\", \"must\", \"will\"]\n",
    "tabulate(cfd, modals, genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Monty Python   '"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"{:{width}}\".format(\"Monty Python\", width=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Writing Results to a File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = open(\"output.txt\", \"w\")\n",
    "words = set(nltk.corpus.genesis.words(\"english-kjv.txt\"))\n",
    "for word in sorted(words):\n",
    "    print(word, file=output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2789"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'2789'"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words)\n",
    "str(len(words))\n",
    "print(str(len(words)), file=output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text Wrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After (5), all (3), is (2), said (4), and (3), done (4), , (1), more (4), is (2), said (4), than (4), done (4), . (1), "
     ]
    }
   ],
   "source": [
    "saying = [\n",
    "    \"After\",\n",
    "    \"all\",\n",
    "    \"is\",\n",
    "    \"said\",\n",
    "    \"and\",\n",
    "    \"done\",\n",
    "    \",\",\n",
    "    \"more\",\n",
    "    \"is\",\n",
    "    \"said\",\n",
    "    \"than\",\n",
    "    \"done\",\n",
    "    \".\",\n",
    "]\n",
    "for word in saying:\n",
    "    print(word, \"(\" + str(len(word)) + \"),\", end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After (5) all (3) is (2) said (4) and (3) done (4) , (1) more (4) is\n",
      "(2) said (4) than (4) done (4) . (1)\n"
     ]
    }
   ],
   "source": [
    "from textwrap import fill\n",
    "\n",
    "pieces = [f\"{word} ({len(word)})\" for word in saying]\n",
    "output = \" \".join(pieces)\n",
    "wrapped = fill(output)\n",
    "print(wrapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
